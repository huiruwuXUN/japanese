{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60020bf9-29a1-4d83-815f-d356514108ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TextSR' object has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoints/model_best.pth\u001b[39m\u001b[38;5;124m'\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# If the loaded object is a state_dict, load it into the model\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Set model to evaluation mode\u001b[39;00m\n\u001b[0;32m     38\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TextSR' object has no attribute 'model'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from src.interfaces.super_resolution import TextSR  # Import the actual model architecture\n",
    "\n",
    "import yaml\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "# Load config and args from YAML file\n",
    "with open('C:/Users/vaibh/TextZoom/src/config/super_resolution.yaml', 'r') as file:\n",
    "    config = edict(yaml.safe_load(file))\n",
    "\n",
    "# Assume args are loaded similarly, or manually define them as needed\n",
    "args = edict({\n",
    "    'syn': False,\n",
    "    'rec': 'aster',\n",
    "    'demo_dir': './demo',\n",
    "    'mixed': False,\n",
    "    'resume': None, \n",
    "    'batch_size': 32,\n",
    "    'test_data_dir': \"C:/Users/vaibh/TextZoom/resized_images\",\n",
    "    'vis_dir': \"C:/Users/vaibh/TextZoom/seg_image_preprocess\",\n",
    "    'mask': False \n",
    "})\n",
    "\n",
    "# Create an instance of the model with config and args\n",
    "model = TextSR(config=config, args=args)\n",
    "# Load the state_dict into the model\n",
    "state_dict = torch.load('checkpoints/model_best.pth', map_location=torch.device('cpu'))\n",
    "\n",
    "# If the loaded object is a state_dict, load it into the model\n",
    "model.model.load_state_dict(state_dict)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# Image preprocessing function\n",
    "def preprocess_image(image_path):\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((32, 128)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    img = transform(img).unsqueeze(0)\n",
    "    return img\n",
    "\n",
    "# Image enhancement function\n",
    "def enhance_image(model, image_path):\n",
    "    img_tensor = preprocess_image(image_path)\n",
    "    \n",
    "    # Forward pass through the model\n",
    "    with torch.no_grad():\n",
    "        enhanced_img = model(img_tensor)\n",
    "    \n",
    "    # Post-process the output\n",
    "    enhanced_img_np = enhanced_img.squeeze().cpu().numpy()\n",
    "    enhanced_img_np = np.transpose(enhanced_img_np, (1, 2, 0))  # Convert back to HWC\n",
    "    enhanced_img_np = (enhanced_img_np * 255).astype(np.uint8)  # Convert to uint8\n",
    "    \n",
    "    return enhanced_img_np\n",
    "\n",
    "# Function to process all images in a folder and save them in another folder\n",
    "def process_folder(input_folder, output_folder):\n",
    "    # Create output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    # Loop through all images in the input folder\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(('.jpg', '.png', '.jpeg')):  # Filter image files\n",
    "            image_path = os.path.join(input_folder, filename)\n",
    "            enhanced_image = enhance_image(model, image_path)\n",
    "            \n",
    "            # Save the enhanced image in the output folder\n",
    "            output_path = os.path.join(output_folder, filename)\n",
    "            cv2.imwrite(output_path, enhanced_image)\n",
    "            print(f\"Processed and saved: {output_path}\")\n",
    "\n",
    "# Define input and output folder paths\n",
    "input_folder = 'resized_images'\n",
    "output_folder = 'textzoom_transformation'\n",
    "\n",
    "# Process images from input folder and save them to output folder\n",
    "process_folder(input_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe704f6-b038-4b1c-aa0a-da70f7615265",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
