{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65aaa710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def display(img):\n",
    "    cv2.imshow(\"1\", img)\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "\n",
    "# def clean_background(img_input):\n",
    "#     hsv = cv2.cvtColor(img_input, cv2.COLOR_BGR2HSV)\n",
    "#     lower_black = np.array([0, 0, 0])\n",
    "#     upper_black = np.array([180, 255, 130])\n",
    "#     mask = cv2.inRange(hsv, lower_black, upper_black)\n",
    "#     cleaned_img = img_input.copy()\n",
    "#     cleaned_img[mask == 0] = [255, 255, 255]\n",
    "#     return cleaned_img\n",
    "\n",
    "\n",
    "def image_preprocess(img_input):\n",
    "    cleaned = clean_background(img_input)\n",
    "    gray_img = cv2.cvtColor(cleaned, cv2.COLOR_BGR2GRAY)\n",
    "    gray_img = cv2.GaussianBlur(gray_img, (3, 3), 3)\n",
    "    _, img = cv2.threshold(gray_img, 0, 255, cv2.THRESH_OTSU)\n",
    "    _, img = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_CROSS, (3, 3))\n",
    "    img = cv2.erode(img, kernel)\n",
    "    return img, cleaned\n",
    "\n",
    "\n",
    "def get_split_line(img, projection_col):\n",
    "    split_line_list = []\n",
    "    flag = False\n",
    "    start = 0\n",
    "    end = 0\n",
    "    for i in range(len(projection_col)):\n",
    "        if not flag and projection_col[i] > 0:\n",
    "            flag = True\n",
    "            start = i\n",
    "        elif flag and (projection_col[i] == 0 or i == len(projection_col) - 1):\n",
    "            flag = False\n",
    "            end = i\n",
    "            if end - start < 15:\n",
    "                flag = True\n",
    "                continue\n",
    "            else:\n",
    "                split_line_list.append((start, end))\n",
    "    return split_line_list\n",
    "\n",
    "\n",
    "def get_contours(img):\n",
    "    contour_list = []\n",
    "    contours, _ = cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    for cnt in contours:\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        area = w * h\n",
    "        if area < 30 or w < 3 or h < 3:\n",
    "            continue  # Filter out small noise\n",
    "        contour_list.append((x, y, w, h))\n",
    "    # Merge vertically adjacent small boxes into a single character\n",
    "    contour_list = merge_small_boxes_vertically(contour_list)\n",
    "    return contour_list\n",
    "\n",
    "\n",
    "def merge_small_boxes_vertically(boxes, gap_thresh=15, x_align_thresh=18, merged_hw_ratio_thresh=(0.7, 4.8)):\n",
    "    if not boxes:\n",
    "        return []\n",
    "\n",
    "    boxes = sorted(boxes, key=lambda b: b[1])  # Sort top to bottom by y\n",
    "    merged = []\n",
    "    used = [False] * len(boxes)\n",
    "\n",
    "    for i in range(len(boxes)):\n",
    "        if used[i]:\n",
    "            continue\n",
    "        x1, y1, w1, h1 = boxes[i]\n",
    "        merged_box = [x1, y1, w1, h1]\n",
    "\n",
    "        for j in range(i + 1, len(boxes)):\n",
    "            if used[j]:\n",
    "                continue\n",
    "            x2, y2, w2, h2 = boxes[j]\n",
    "\n",
    "            vertical_gap = y2 - (y1 + h1)\n",
    "            x_align = abs(x1 - x2)\n",
    "            merged_x = min(x1, x2)\n",
    "            merged_y = min(y1, y2)\n",
    "            merged_w = max(x1 + w1, x2 + w2) - merged_x\n",
    "            merged_h = max(y1 + h1, y2 + h2) - merged_y\n",
    "            hw_ratio = merged_h / merged_w if merged_w != 0 else 999\n",
    "\n",
    "            # Merge if three conditions are met\n",
    "            if 0 <= vertical_gap < gap_thresh and x_align < x_align_thresh and \\\n",
    "               merged_hw_ratio_thresh[0] < hw_ratio < merged_hw_ratio_thresh[1]:\n",
    "                merged_box = [merged_x, merged_y, merged_w, merged_h]\n",
    "                used[j] = True\n",
    "\n",
    "        merged.append(tuple(merged_box))\n",
    "        used[i] = True\n",
    "\n",
    "    return merged\n",
    "\n",
    "\n",
    "def sort_merge(contour_row):\n",
    "    contour_row = sorted(contour_row, key=lambda x: x[1])  # Sort top to bottom (for vertical layout)\n",
    "    i = 0\n",
    "    for _ in contour_row:\n",
    "        if i == len(contour_row) - 1 or contour_row[i][0] == -1:\n",
    "            break\n",
    "        rectR = contour_row[i + 1]\n",
    "        rectL = contour_row[i]\n",
    "        ovlp = rectL[1] + rectL[3] - rectR[1]\n",
    "        dist = abs((rectR[1] + rectR[3] / 2) - (rectL[1] - rectL[3] / 2))\n",
    "        h_L = rectL[1] + rectL[3]\n",
    "        h_R = rectR[1] + rectR[3]\n",
    "        span = max(h_R, h_L) - rectL[1]\n",
    "        nmovlp = (ovlp / rectL[3] + ovlp / rectR[3]) / 2 - dist / span / 8\n",
    "        if nmovlp > 0:\n",
    "            x = min(rectL[0], rectR[0])\n",
    "            y = rectL[1]\n",
    "            w = max(rectL[0] + rectL[2], rectR[0] + rectR[2]) - x\n",
    "            h = max(h_L, h_R) - y\n",
    "            contour_row[i] = (x, y, w, h)\n",
    "            contour_row.pop(i + 1)\n",
    "            contour_row.append((-1, -1, -1, -1))\n",
    "            i -= 1\n",
    "        i += 1\n",
    "    return contour_row\n",
    "\n",
    "\n",
    "def combine_verticalLine(contour_row):\n",
    "    return contour_row  # Optional: retained for vertical layout; vertical splits are generally not a problem\n",
    "\n",
    "\n",
    "def split_oversizeWidth(contour_row):\n",
    "    i = 0\n",
    "    for _ in contour_row:\n",
    "        rect = contour_row[i]\n",
    "        if rect[3] * 1.0 / rect[2] > 1.8:  # Tall aspect ratio may indicate merged characters\n",
    "            y_new = int(rect[1] + rect[3] / 2 + 1)\n",
    "            x_new = rect[0]\n",
    "            h_new = rect[1] + rect[3] - y_new\n",
    "            w_new = rect[2]\n",
    "            contour_row[i] = (rect[0], rect[1], rect[2], int(rect[3] / 2))\n",
    "            contour_row.insert(i + 1, (x_new, y_new, w_new, h_new))\n",
    "        i += 1\n",
    "    return contour_row\n",
    "\n",
    "\n",
    "def get_segmentation_result(img, img_input, cleaned_img, save_dir=\"char_output\"):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    projection_col = cv2.reduce(img, 0, cv2.REDUCE_SUM, dtype=cv2.CV_32S)  # Vertical projection\n",
    "    projection_col = projection_col.flatten()\n",
    "    split_line_list = get_split_line(img, projection_col)\n",
    "\n",
    "    # Process each column from right to left\n",
    "    split_line_list = sorted(split_line_list, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    segmentation_result = []\n",
    "    char_index = 1\n",
    "    h_img, w_img = img_input.shape[:2]\n",
    "\n",
    "    for i in split_line_list:\n",
    "        img_col = img[:, i[0]:i[1]]\n",
    "        contour_col = get_contours(img_col)\n",
    "        contour_col = sort_merge(contour_col)\n",
    "        contour_col = split_oversizeWidth(contour_col)\n",
    "        contour_col = combine_verticalLine(contour_col)\n",
    "        segmentation_result.append(contour_col)\n",
    "\n",
    "        for (x, y, w, h) in contour_col:\n",
    "            x_abs = x + i[0]  # Correct x coordinate\n",
    "            x_end = min(x_abs + w, w_img)\n",
    "            y_end = min(y + h, h_img)\n",
    "\n",
    "            if x_abs < x_end and y < y_end:\n",
    "                # Add padding (white space)\n",
    "                pad = 3  # Try adjusting from 2 to 5\n",
    "                x_pad1 = max(x_abs - pad, 0)\n",
    "                y_pad1 = max(y - pad, 0)\n",
    "                x_pad2 = min(x_abs + w + pad, w_img)\n",
    "                y_pad2 = min(y + h + pad, h_img)\n",
    "\n",
    "                char_img = cleaned_img[y_pad1:y_pad2, x_pad1:x_pad2]\n",
    "                if char_img is not None and char_img.size > 0:\n",
    "                    save_path = os.path.join(save_dir, f\"char_{char_index:03d}.jpg\")\n",
    "                    cv2.imwrite(save_path, char_img)\n",
    "                    char_index += 1\n",
    "\n",
    "                # Draw enlarged red box for visualization\n",
    "                cv2.rectangle(img_input, (x_pad1, y_pad1), (x_pad2, y_pad2), (0, 0, 255))\n",
    "\n",
    "    return segmentation_result\n",
    "\n",
    "\n",
    "# ========== Main Program ==========\n",
    "\n",
    "# pic_path = 'J5.png'\n",
    "# img_input = cv2.imread(pic_path, 1)\n",
    "# img, cleaned_img = image_preprocess(img_input)\n",
    "# cleaned_img_copy = cleaned_img.copy()\n",
    "\n",
    "# segmentation_result = get_segmentation_result(\n",
    "#     img, cleaned_img_copy, cleaned_img, save_dir=\"char_output\"\n",
    "# )\n",
    "\n",
    "# cv2.imwrite(\"segmented_visual.jpg\", cleaned_img_copy)\n",
    "# cv2.imwrite(\"cleaned_background.jpg\", cleaned_img)\n",
    "\n",
    "# display(cleaned_img_copy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e8df7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "def display(img):\n",
    "    cv2.imshow(\"1\", img)\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "\n",
    "def adaptive_threshold(image):\n",
    "    \"\"\"\n",
    "    To handle low-quality images, an adaptive thresholding approach is introduced.\n",
    "    - When the image contains a lot of noise, the threshold is relaxed.\n",
    "    - Bright areas are additionally identified using binarization.\n",
    "    \"\"\"\n",
    "    height, width, _ = image.shape\n",
    "    blurred = cv2.GaussianBlur(image, (5, 5), 0)\n",
    "    gray = cv2.cvtColor(blurred, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    std_dev = np.std(gray)\n",
    "\n",
    "    qerr1 = int(30 + (std_dev / 5))\n",
    "    qerr2 = int(150 + (std_dev / 3))\n",
    "\n",
    "    print(f\"Adaptive Thresholds -> qerr1: {qerr1}, qerr2: {qerr2}, Std Dev: {std_dev:.2f}\")\n",
    "\n",
    "    _, bright_mask = cv2.threshold(gray, 220, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    r_arr, g_arr, b_arr = np.zeros(256, dtype=int), np.zeros(256, dtype=int), np.zeros(256, dtype=int)\n",
    "\n",
    "    for i in range(height):\n",
    "        for j in range(width):\n",
    "            b, g, r = image[i, j]\n",
    "            r_arr[r] += 1\n",
    "            g_arr[g] += 1\n",
    "            b_arr[b] += 1\n",
    "\n",
    "    r_mean = np.argmax(r_arr)\n",
    "    g_mean = np.argmax(g_arr)\n",
    "    b_mean = np.argmax(b_arr)\n",
    "\n",
    "    print(f\"Detected color mode -> R: {r_mean}, G: {g_mean}, B: {b_mean}\")\n",
    "\n",
    "    new_image = image.copy()\n",
    "\n",
    "    for i in range(height):\n",
    "        for j in range(width):\n",
    "            b, g, r = new_image[i, j]\n",
    "            err = math.sqrt((b - b_mean) ** 2 + (g - g_mean) ** 2 + (r - r_mean) ** 2)\n",
    "            if err < qerr1:\n",
    "                new_image[i, j] = np.array([255, 255, 255], dtype=np.uint8)\n",
    "\n",
    "    for i in range(height):\n",
    "        for j in range(width):\n",
    "            b, g, r = new_image[i, j]\n",
    "            err = math.sqrt((b - 255) ** 2 + (g - 255) ** 2 + (r - 255) ** 2)\n",
    "            if err < qerr2:\n",
    "                new_image[i, j] = np.array([255, 255, 255], dtype=np.uint8)\n",
    "\n",
    "    for i in range(height):\n",
    "        for j in range(width):\n",
    "            if bright_mask[i, j] == 255:\n",
    "                new_image[i, j] = np.array([255, 255, 255], dtype=np.uint8)\n",
    "\n",
    "    return new_image\n",
    "\n",
    "\n",
    "def image_preprocess(img_input):\n",
    "    cleaned = adaptive_threshold(img_input)\n",
    "\n",
    "    gray_img = cv2.cvtColor(cleaned, cv2.COLOR_BGR2GRAY)\n",
    "    gray_img = cv2.GaussianBlur(gray_img, (3, 3), 3)\n",
    "\n",
    "    _, img = cv2.threshold(gray_img, 0, 255, cv2.THRESH_OTSU)\n",
    "    _, img = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_CROSS, (3, 3))\n",
    "    img = cv2.erode(img, kernel)\n",
    "\n",
    "    return img, cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "136f2a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaptive Thresholds -> qerr1: 37, qerr2: 162, Std Dev: 36.33\n",
      "Detected color mode -> R: 224, G: 212, B: 183\n",
      "Processed J257..png\n",
      "Adaptive Thresholds -> qerr1: 36, qerr2: 161, Std Dev: 34.80\n",
      "Detected color mode -> R: 228, G: 215, B: 191\n",
      "Processed J241..png\n",
      "Adaptive Thresholds -> qerr1: 37, qerr2: 162, Std Dev: 36.54\n",
      "Detected color mode -> R: 244, G: 215, B: 187\n",
      "Processed J236..png\n",
      "Adaptive Thresholds -> qerr1: 35, qerr2: 159, Std Dev: 28.22\n",
      "Detected color mode -> R: 244, G: 211, B: 183\n",
      "Processed J237..png\n",
      "Adaptive Thresholds -> qerr1: 37, qerr2: 161, Std Dev: 35.52\n",
      "Detected color mode -> R: 219, G: 206, B: 175\n",
      "Processed J295..png\n",
      "Adaptive Thresholds -> qerr1: 40, qerr2: 166, Std Dev: 50.14\n",
      "Detected color mode -> R: 225, G: 213, B: 184\n",
      "Processed J240..png\n",
      "Adaptive Thresholds -> qerr1: 38, qerr2: 164, Std Dev: 44.82\n",
      "Detected color mode -> R: 218, G: 206, B: 178\n",
      "Processed J305..png\n",
      "Adaptive Thresholds -> qerr1: 36, qerr2: 160, Std Dev: 31.14\n",
      "Detected color mode -> R: 228, G: 217, B: 188\n",
      "Processed J11.png\n",
      "Adaptive Thresholds -> qerr1: 39, qerr2: 165, Std Dev: 47.63\n",
      "Detected color mode -> R: 223, G: 209, B: 179\n",
      "Processed J318..png\n",
      "Adaptive Thresholds -> qerr1: 37, qerr2: 162, Std Dev: 38.08\n",
      "Detected color mode -> R: 225, G: 214, B: 185\n",
      "Processed J4.png\n",
      "Adaptive Thresholds -> qerr1: 37, qerr2: 162, Std Dev: 38.15\n",
      "Detected color mode -> R: 224, G: 213, B: 184\n",
      "Processed J5.png\n",
      "Adaptive Thresholds -> qerr1: 38, qerr2: 164, Std Dev: 43.97\n",
      "Detected color mode -> R: 223, G: 211, B: 178\n",
      "Processed J271..png\n",
      "Adaptive Thresholds -> qerr1: 37, qerr2: 162, Std Dev: 37.18\n",
      "Detected color mode -> R: 223, G: 211, B: 182\n",
      "Processed J7.png\n",
      "Adaptive Thresholds -> qerr1: 38, qerr2: 163, Std Dev: 41.07\n",
      "Detected color mode -> R: 220, G: 209, B: 182\n",
      "Processed J251..png\n",
      "Adaptive Thresholds -> qerr1: 36, qerr2: 160, Std Dev: 30.31\n",
      "Detected color mode -> R: 234, G: 230, B: 214\n",
      "Processed J104 cropped..png\n",
      "Adaptive Thresholds -> qerr1: 36, qerr2: 160, Std Dev: 30.77\n",
      "Detected color mode -> R: 227, G: 216, B: 188\n",
      "Processed J6.png\n",
      "Adaptive Thresholds -> qerr1: 34, qerr2: 158, Std Dev: 24.01\n",
      "Detected color mode -> R: 235, G: 221, B: 194\n",
      "Processed J2.png\n",
      "Adaptive Thresholds -> qerr1: 37, qerr2: 162, Std Dev: 37.54\n",
      "Detected color mode -> R: 223, G: 212, B: 182\n",
      "Processed J285..png\n",
      "Adaptive Thresholds -> qerr1: 40, qerr2: 167, Std Dev: 52.93\n",
      "Detected color mode -> R: 224, G: 214, B: 185\n",
      "Processed J3.png\n",
      "Adaptive Thresholds -> qerr1: 33, qerr2: 156, Std Dev: 18.98\n",
      "Detected color mode -> R: 249, G: 245, B: 229\n",
      "Processed J335..png\n",
      "Adaptive Thresholds -> qerr1: 34, qerr2: 157, Std Dev: 23.85\n",
      "Detected color mode -> R: 231, G: 216, B: 190\n",
      "Processed J1.png\n",
      "Adaptive Thresholds -> qerr1: 36, qerr2: 160, Std Dev: 31.54\n",
      "Detected color mode -> R: 221, G: 213, B: 184\n",
      "Processed J289..png\n",
      "Adaptive Thresholds -> qerr1: 38, qerr2: 163, Std Dev: 41.74\n",
      "Detected color mode -> R: 232, G: 230, B: 210\n",
      "Processed J110.png\n",
      "Adaptive Thresholds -> qerr1: 39, qerr2: 165, Std Dev: 45.65\n",
      "Detected color mode -> R: 220, G: 209, B: 179\n",
      "Processed J273..png\n",
      "Adaptive Thresholds -> qerr1: 35, qerr2: 159, Std Dev: 27.80\n",
      "Detected color mode -> R: 217, G: 203, B: 167\n",
      "Processed J336..png\n",
      "Adaptive Thresholds -> qerr1: 36, qerr2: 161, Std Dev: 33.14\n",
      "Detected color mode -> R: 217, G: 215, B: 199\n",
      "Processed J109.png\n",
      "Adaptive Thresholds -> qerr1: 41, qerr2: 168, Std Dev: 55.40\n",
      "Detected color mode -> R: 244, G: 213, B: 183\n",
      "Processed J265..png\n",
      "Adaptive Thresholds -> qerr1: 36, qerr2: 160, Std Dev: 30.95\n",
      "Detected color mode -> R: 229, G: 227, B: 206\n",
      "Processed J108.png\n",
      "Adaptive Thresholds -> qerr1: 34, qerr2: 158, Std Dev: 24.89\n",
      "Detected color mode -> R: 228, G: 217, B: 189\n",
      "Processed J249..png\n",
      "Adaptive Thresholds -> qerr1: 38, qerr2: 164, Std Dev: 42.78\n",
      "Detected color mode -> R: 222, G: 209, B: 176\n",
      "Processed J286..png\n",
      "Adaptive Thresholds -> qerr1: 44, qerr2: 174, Std Dev: 73.92\n",
      "Detected color mode -> R: 225, G: 214, B: 185\n",
      "Processed J253..png\n",
      "Adaptive Thresholds -> qerr1: 40, qerr2: 166, Std Dev: 50.44\n",
      "Detected color mode -> R: 225, G: 214, B: 184\n",
      "Processed J252..png\n",
      "Adaptive Thresholds -> qerr1: 35, qerr2: 158, Std Dev: 25.47\n",
      "Detected color mode -> R: 214, G: 213, B: 188\n",
      "Processed J182..png\n",
      "Adaptive Thresholds -> qerr1: 37, qerr2: 162, Std Dev: 38.57\n",
      "Detected color mode -> R: 209, G: 202, B: 174\n",
      "Processed J301..png\n",
      "Adaptive Thresholds -> qerr1: 38, qerr2: 164, Std Dev: 42.30\n",
      "Detected color mode -> R: 226, G: 215, B: 187\n",
      "Processed J244..png\n",
      "Adaptive Thresholds -> qerr1: 35, qerr2: 159, Std Dev: 29.91\n",
      "Detected color mode -> R: 215, G: 206, B: 178\n",
      "Processed J268..png\n",
      "Adaptive Thresholds -> qerr1: 38, qerr2: 164, Std Dev: 42.80\n",
      "Detected color mode -> R: 211, G: 188, B: 142\n",
      "Processed J8.png\n",
      "Adaptive Thresholds -> qerr1: 35, qerr2: 159, Std Dev: 28.14\n",
      "Detected color mode -> R: 227, G: 216, B: 186\n",
      "Processed J248..png\n",
      "Adaptive Thresholds -> qerr1: 35, qerr2: 159, Std Dev: 29.07\n",
      "Detected color mode -> R: 215, G: 203, B: 174\n",
      "Processed J296..png\n",
      "Adaptive Thresholds -> qerr1: 37, qerr2: 161, Std Dev: 35.61\n",
      "Detected color mode -> R: 225, G: 217, B: 188\n",
      "Processed J238..png\n",
      "Adaptive Thresholds -> qerr1: 39, qerr2: 165, Std Dev: 45.55\n",
      "Detected color mode -> R: 239, G: 205, B: 177\n",
      "Processed J275..png\n",
      "Adaptive Thresholds -> qerr1: 35, qerr2: 159, Std Dev: 29.51\n",
      "Detected color mode -> R: 233, G: 230, B: 214\n",
      "Processed J104 full..png\n",
      "Adaptive Thresholds -> qerr1: 36, qerr2: 160, Std Dev: 32.47\n",
      "Detected color mode -> R: 226, G: 215, B: 185\n",
      "Processed J259..png\n",
      "Adaptive Thresholds -> qerr1: 34, qerr2: 157, Std Dev: 23.85\n",
      "Detected color mode -> R: 231, G: 216, B: 190\n",
      "Processed 1.png\n",
      "Adaptive Thresholds -> qerr1: 36, qerr2: 160, Std Dev: 31.90\n",
      "Detected color mode -> R: 227, G: 216, B: 189\n",
      "Processed J254..png\n",
      "Adaptive Thresholds -> qerr1: 35, qerr2: 158, Std Dev: 26.79\n",
      "Detected color mode -> R: 220, G: 217, B: 200\n",
      "Processed J184..png\n",
      "Adaptive Thresholds -> qerr1: 38, qerr2: 164, Std Dev: 43.92\n",
      "Detected color mode -> R: 219, G: 206, B: 176\n",
      "Processed J307..png\n",
      "Adaptive Thresholds -> qerr1: 39, qerr2: 166, Std Dev: 48.26\n",
      "Detected color mode -> R: 225, G: 217, B: 190\n",
      "Processed J242..png\n"
     ]
    }
   ],
   "source": [
    "input_folder = \"input_images\"  # ここに元画像を全部入れておく\n",
    "output_root = \"processed_image\"  # ここに全部保存したい\n",
    "\n",
    "if not os.path.exists(output_root):\n",
    "    os.makedirs(output_root)\n",
    "\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".bmp\")):\n",
    "        img_path = os.path.join(input_folder, filename)\n",
    "        img_input = cv2.imread(img_path, 1)\n",
    "\n",
    "        if img_input is None:\n",
    "            print(f\"Warning: Failed to load {filename}\")\n",
    "            continue\n",
    "\n",
    "        img, cleaned_img = image_preprocess(img_input)\n",
    "        cleaned_img_copy = cleaned_img.copy()\n",
    "\n",
    "        # フォルダ名をファイル名から作成（拡張子を除く）\n",
    "        base_name = os.path.splitext(filename)[0]\n",
    "        save_dir = os.path.join(output_root, base_name)\n",
    "\n",
    "        segmentation_result = get_segmentation_result(\n",
    "            img, cleaned_img_copy, cleaned_img, save_dir=save_dir\n",
    "        )\n",
    "\n",
    "        # 保存（必要ならコメント外して）\n",
    "        cv2.imwrite(os.path.join(save_dir, \"segmented_visual.jpg\"), cleaned_img_copy)\n",
    "        cv2.imwrite(os.path.join(save_dir, \"cleaned_background.jpg\"), cleaned_img)\n",
    "\n",
    "        print(f\"Processed {filename}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
